{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e606c5f-f004-4f6f-b64e-a093ef771c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a183c2a-dd46-4e12-88c5-b47fa61d4eda",
   "metadata": {},
   "source": [
    "### 安装环境后需要重启虚拟机核心"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3fc4293-63f4-465b-836d-61022b503db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install Pillow==10.1.0 torch==2.1.2 torchvision==0.16.2 transformers==4.40.0 sentencepiece==0.1.99 accelerate==0.30.1 bitsandbytes==0.43.1 peft==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d81b07a-8ff4-4481-b8d7-8e9e98cc53b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.49s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "model_path = '/mnt/ceph/develop/jiawei/ComfyUI/models/LLM/MiniCPMv2_6-prompt-generator'\n",
    "attention = 'sdpa'\n",
    "precision = 'fp16'\n",
    "dtype = {\"bf16\": torch.bfloat16, \"fp16\": torch.float16, \"fp32\": torch.float32}[precision]\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, attn_implementation=attention,\n",
    "                                                         torch_dtype=dtype, load_in_4bit=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6453b16-5ebd-438b-b9b8-c8552c98d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from typing import Union, List\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "import PIL.Image\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "\n",
    "def pad_frame(img, scale):\n",
    "    _, _, h, w = img.shape\n",
    "    tmp = max(32, int(32 / scale))\n",
    "    ph = ((h - 1) // tmp + 1) * tmp\n",
    "    pw = ((w - 1) // tmp + 1) * tmp\n",
    "    padding = (0,  pw - w, 0, ph - h)\n",
    "    return F.pad(img, padding)\n",
    "\n",
    "def pad_video_of_np(samples, w, h):\n",
    "    print(f\"samples dtype:{samples.dtype}\")\n",
    "    print(f\"samples shape:{samples.shape}\")\n",
    "    output = []\n",
    "    # [f, c, h, w]\n",
    "    for b in range(samples.shape[0]):\n",
    "        frame = samples[b : b + 1]\n",
    "        frame = pad_frame(frame, 1).to(dtype=samples.dtype)\n",
    "        \n",
    "        frame = F.interpolate(frame, size=(h, w))\n",
    "        output.append(frame.squeeze(0)) # (to [f, w, h, c])\n",
    "\n",
    "    image_np = VaeImageProcessor.pt_to_numpy(torch.stack(output))  # (to [49, 512, 480, 3])\n",
    "    image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "    return image_pil\n",
    "\n",
    "\n",
    "def split_video(video_path, output_dir, start_time, end_time, features_frame, w=720 , h=960 ,chunk_duration=3):\n",
    "    # Open the video file\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)  # Get the frames per second\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))  # Total frames in the video\n",
    "    output_files = []\n",
    "    # Calculate start and end frames based on time\n",
    "    start_frame = int(start_time * fps)\n",
    "    end_frame = int(end_time * fps)\n",
    "\n",
    "    video_capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)  # Set video position to start frame\n",
    "\n",
    "    current_frame = start_frame\n",
    "    chunk_index = 0\n",
    "\n",
    "    while current_frame <= end_frame:\n",
    "        chunk_frames = []\n",
    "        for _ in range(int(chunk_duration * fps)):\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break  \n",
    "            frame_rgb = frame[..., ::-1]\n",
    "            \n",
    "            # 创建一个新数组，确保 stride 是正的\n",
    "            frame_rgb = frame_rgb.copy()\n",
    "            tensor = torch.from_numpy(frame_rgb).float().to(\"cpu\", non_blocking=True).float() / 255.0\n",
    "            chunk_frames.append(\n",
    "                tensor.permute(2, 0, 1)\n",
    "            )  # to [c, h, w,]\n",
    "            current_frame += 1\n",
    "\n",
    "        if len(chunk_frames) == 0:\n",
    "            break\n",
    "            \n",
    "        # If the last chunk is shorter than expected, repeat the last frame\n",
    "        while len(chunk_frames) < int(chunk_duration * fps):\n",
    "            chunk_frames.append(chunk_frames[-1])\n",
    "            \n",
    "        pt_frame = torch.from_numpy(np.stack(chunk_frames))  # to [f, c, h, w]\n",
    "         \n",
    "        chunk_images = pad_video_of_np(pt_frame,w,h)\n",
    "     \n",
    "        # Save the chunk as a video\n",
    "        output_file = os.path.join(output_dir, f'chunk_{features_frame}_{chunk_index}.mp4')\n",
    "        print(output_file)\n",
    "        save_chunk_as_video(chunk_images, output_file, fps=math.ceil((len(chunk_images) - 1) / 6))\n",
    "        output_files.append(output_file)\n",
    "        chunk_index += 1\n",
    "\n",
    "    video_capture.release()\n",
    "    return output_files\n",
    "    \n",
    " \n",
    "    \n",
    "def save_chunk_as_video(tensor: Union[List[np.ndarray], List[PIL.Image.Image]],video_path, fps: int = 8):\n",
    "    os.makedirs(os.path.dirname(video_path), exist_ok=True)\n",
    "    export_to_video(tensor, video_path, fps=fps)\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57626e8c-6d29-434c-bb60-b1cadb9450ab",
   "metadata": {},
   "source": [
    "#### 使用gpt重新给这个场景增加描述\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce58a0f-7a25-4b45-b079-46778743cc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65afd641-6049-4597-a652-9e08dd5c9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # api_key=\"YOUR_API_KEY\",\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    import base64\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "def caption_chunk_gpt(video_path, features_caption):\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    ret, first_frame = video_capture.read()\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_capture.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)\n",
    "    ret, last_frame = video_capture.read()\n",
    "\n",
    "    # Get the frame rate (frames per second)\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the duration of the video in seconds\n",
    "    video_duration = total_frames / fps\n",
    "    \n",
    "    video_capture.release()\n",
    "    \n",
    "    first_frame_rgb = first_frame[..., ::-1] \n",
    "    first_frame_rgb = first_frame_rgb.copy()\n",
    "\n",
    "    last_frame_rgb = last_frame[..., ::-1] \n",
    "    last_frame_rgb = last_frame_rgb.copy()\n",
    "    \n",
    "    first_frame_pil_image = Image.fromarray(first_frame_rgb)\n",
    "\n",
    "    last_frame_pil_image = Image.fromarray(last_frame_rgb)\n",
    "\n",
    "    prompt = \"\"\"Follow these steps to create a Midjourney-style long prompt for generating high-quality images: \n",
    "            1. The prompt should include rich details, vivid scenes, and composition information, capturing the important elements that make up the scene. \n",
    "            2. You can appropriately add some details to enhance the vividness and richness of the content, while ensuring that the long prompt does not exceed 256 tokens,you should only return prompt，itself without any additional information\"\"\"\n",
    "\n",
    "    # Prepare the input for the chat method\n",
    "    msgs = [{\"role\": \"user\", \"content\": [first_frame_pil_image, prompt]}]\n",
    "    # Use the chat method\n",
    "    first_frame_generated_text = model.chat(\n",
    "        image=[first_frame_pil_image],\n",
    "        msgs=msgs,\n",
    "        tokenizer=tokenizer,\n",
    "        processor=processor,\n",
    "        max_new_tokens=2048,\n",
    "        sampling=False,\n",
    "        num_beams=3\n",
    "    ) \n",
    "    \n",
    "    # Prepare the input for the chat method\n",
    "    msgs = [{\"role\": \"user\", \"content\": [last_frame_pil_image, prompt]}]\n",
    "    # Use the chat method\n",
    "    last_frame_generated_text = model.chat(\n",
    "        image=[last_frame_pil_image],\n",
    "        msgs=msgs,\n",
    "        tokenizer=tokenizer,\n",
    "        processor=processor,\n",
    "        max_new_tokens=2048,\n",
    "        sampling=False,\n",
    "        num_beams=3\n",
    "    ) \n",
    "    # Sample dictionary of image captions (this should be generated based on your video analysis)\n",
    "    image_captions = {\n",
    "        \"0\": features_caption,\n",
    "        \"1\": first_frame_generated_text,\n",
    "        video_duration: last_frame_generated_text\n",
    "    }\n",
    "    \n",
    "    # Convert image_captions dictionary into a format suitable for new_captions\n",
    "    new_captions = \"\\n\".join([f\"{time}: '{description}'\" for time, description in image_captions.items()])\n",
    "\n",
    "    caption_summary_prompt = f\"\"\"We extracted several frames from this video and described\n",
    "each frame using an image understanding model, stored in the dictionary variable ‘image_captions: Dict[str: str]‘.\n",
    "In ‘image_captions‘, the key is the second at which the image appears in the video, and the value is a detailed description\n",
    "of the image at that moment. Please describe the content of this video in as much detail as possible, based on the\n",
    "information provided by ‘image_captions‘, including the objects, scenery, animals, characters, and camera\n",
    "movements within the video. \\n image_captions={new_captions}\\n\\n\n",
    "You should output your summary directly, and not mention\n",
    "variables like ‘image_captions‘ in your response.\n",
    "Do not include ‘\\\\n’ and the word ’video’ in your response.\n",
    "Do not use introductory phrases such as: \\\"The video presents\\\", \\\"The video depicts\\\", \\\"This video showcases\\\",\n",
    "\\\"The video captures\\\" and so on.\\n Please start the description with the video content directly, such as \\\"A man\n",
    "first sits in a chair, then stands up and walks to the kitchen....\\\"\\n Do not use phrases like: \\\"as the video\n",
    "progressed\\\" and \\\"Throughout the video\\\".\\n Please describe  the content of the video and the changes that occur, in\n",
    "chronological order.\\n Please keep the description of this video within 100 English words.\"\"\"\n",
    "\n",
    "    print(f\"caption_summary_prompt:{caption_summary_prompt}\")\n",
    "    # 减少信息，生成速度可以在0.1秒完成\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"web_search\",\n",
    "            \"web_search\": {\n",
    "                \"enable\": False, \n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4-plus\",\n",
    "        messages=[ {\"role\": \"user\", \"content\": f\"{caption_summary_prompt}\"}],\n",
    "        temperature=0,\n",
    "        tools=tools,\n",
    "        max_tokens=2000,\n",
    "    ) \n",
    "\n",
    "    caption_summary_text = response.choices[0].message.content\n",
    "    print(f\"caption_summary_text:{caption_summary_text}\")\n",
    "    return first_frame_generated_text, last_frame_generated_text, caption_summary_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01901b-2ce9-4287-88b2-f0e13398ad87",
   "metadata": {},
   "source": [
    "#### 删除前后时间场景的误差时间 ， 按照3秒每个片段裁剪, fps是原视频的\n",
    "获取第一帧和最后一帧画面，使用gpt整理视频提示词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da84ff3b-15c6-4216-8fb0-fcdf86442219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loadcsv_scene_chunk(scene_csv_path, video_path, videos_output_dir, labels_output_dir):\n",
    "    df = pd.read_csv(scene_csv_path)\n",
    "    \n",
    "    # 计算每个分镜编号的总持续时间\n",
    "    df['持续时间'] = (pd.to_datetime(df['结束时间']) - pd.to_datetime(df['开始时间'])).dt.total_seconds()\n",
    "    grouped_df = df.groupby('分镜')['持续时间'].sum().reset_index()\n",
    "    df = df[df['持续时间'] >= 3]\n",
    "    \n",
    "    df = df.sort_values(by='持续时间', ascending=True)\n",
    "        \n",
    "    # 创建一个新的 DataFrame 来存储行\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    # 为新数据添加列\n",
    "    new_df['chunk_index'] = None\n",
    "    new_df['chunk_path'] = None\n",
    "    new_df['first_frame_generated_text'] = None\n",
    "    new_df['last_frame_generated_text'] = None\n",
    "    new_df['caption_summary_text'] = None\n",
    "    # Iterate over each row in the table\n",
    "    for _, row in df.iterrows():\n",
    "        start_time = row['开始时间']\n",
    "        end_time = row['结束时间']\n",
    "        features_caption = row['特征描述']\n",
    "        features_frame = int(row['特征帧'])\n",
    "        # 将行转换为 DataFrame 并追加到 new_df\n",
    "        new_df = pd.concat([new_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "        # 创建一个新的 DataFrame 来存储冗余行\n",
    "        expanded_df = pd.DataFrame(columns=new_df.columns)\n",
    "        # 将开始时间和结束时间解析为时间对象\n",
    "        start_time_obj = datetime.datetime.strptime(start_time, \"%H:%M:%S,%f\")\n",
    "        end_time_obj = datetime.datetime.strptime(end_time, \"%H:%M:%S,%f\")\n",
    "    \n",
    "        # 计算时间区间的秒数\n",
    "        start_seconds = (start_time_obj.hour * 3600 + start_time_obj.minute * 60 + start_time_obj.second +\n",
    "                         start_time_obj.microsecond / 1000000)\n",
    "        end_seconds = (end_time_obj.hour * 3600 + end_time_obj.minute * 60 + end_time_obj.second +\n",
    "                       end_time_obj.microsecond / 1000000)\n",
    "        start_seconds = start_seconds-1\n",
    "        end_seconds = end_seconds-1\n",
    "        # Split the video for each start and end time\n",
    "        output_files = split_video(video_path, videos_output_dir, start_seconds, end_seconds,features_frame)\n",
    "        for index, path in enumerate(output_files):\n",
    "            first_frame_generated_text, last_frame_generated_text, caption_summary_text = caption_chunk_gpt(path, features_caption)\n",
    "            \n",
    "            # Update the corresponding row in new_df with the generated text values\n",
    "            new_df.at[new_df.index[-1], 'chunk_index'] = f'chunk_{features_frame}_{index}'\n",
    "            new_df.at[new_df.index[-1], 'chunk_path'] = path\n",
    "            new_df.at[new_df.index[-1], 'first_frame_generated_text'] = first_frame_generated_text\n",
    "            new_df.at[new_df.index[-1], 'last_frame_generated_text'] = last_frame_generated_text\n",
    "            new_df.at[new_df.index[-1], 'caption_summary_text'] = caption_summary_text\n",
    "            \n",
    "            lable_file = os.path.join(labels_output_dir, f'chunk_{features_frame}_{index}.txt')\n",
    "            # Open the file in write mode\n",
    "            with open(lable_file, 'w') as file: \n",
    "                file.write(caption_summary_text)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d713d948-cf22-47dc-ad74-933066d459b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_output_folder(output_folder):\n",
    "    if not os.path.isdir(output_folder):\n",
    "        print('warning: the folder{} is not exist'.format(output_folder))\n",
    "        # create srt_folder\n",
    "        os.makedirs(output_folder)\n",
    "        print('create folder', output_folder)\n",
    "\n",
    "def split_video_with_chunk(root_path, video_source):\n",
    "    save_path = f'{root_path}/{video_source}/'\n",
    "    video_path=f'{save_path}/{video_source}.mp4' \n",
    " \n",
    "    scene_csv_path=f'{save_path}/scene/{video_source}_scene_keyframe.csv'\n",
    "    videos_output_dir = f'{save_path}/scene_chunks/videos'\n",
    "    labels_output_dir = f'{save_path}/scene_chunks/labels'\n",
    "\n",
    "    scene_chunks_csv_path = f'{save_path}/scene_chunks/{video_source}_scene_chunk_frame.csv'\n",
    "    check_output_folder(videos_output_dir)\n",
    "\n",
    "    check_output_folder(labels_output_dir)\n",
    "    \n",
    "    new_df = loadcsv_scene_chunk(scene_csv_path,video_path,videos_output_dir, labels_output_dir)\n",
    "    \n",
    "    new_df.to_csv(scene_chunks_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b902e-42ae-4c57-bc11-2978778e9bd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_640152/3613310089.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['持续时间'] = (pd.to_datetime(df['结束时间']) - pd.to_datetime(df['开始时间'])).dt.total_seconds()\n",
      "/tmp/ipykernel_640152/3613310089.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['持续时间'] = (pd.to_datetime(df['结束时间']) - pd.to_datetime(df['开始时间'])).dt.total_seconds()\n",
      "/tmp/ipykernel_640152/3613310089.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  new_df = pd.concat([new_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([60, 3, 360, 540])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/猫和老鼠_只有人物//test_150//scene_chunks/videos/chunk_653_0.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([60, 3, 360, 540])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/猫和老鼠_只有人物//test_150//scene_chunks/videos/chunk_653_1.mp4\n",
      "caption_summary_prompt:We extracted several frames from this video and described\n",
      "each frame using an image understanding model, stored in the dictionary variable ‘image_captions: Dict[str: str]‘.\n",
      "In ‘image_captions‘, the key is the second at which the image appears in the video, and the value is a detailed description\n",
      "of the image at that moment. Please describe the content of this video in as much detail as possible, based on the\n",
      "information provided by ‘image_captions‘, including the objects, scenery, animals, characters, and camera\n",
      "movements within the video. \n",
      " image_captions=0: 'In the heart of a dimly lit laboratory, Tom, the mischievous brown cat, stands poised on a wooden floor. Clad in a pair of red gloves, he wields a green laser pointer, its beam tracing a straight line across the room. Directly ahead of him, a small glass beaker sits on a stand, filled to the brim with a mysterious red liquid. The beaker's position and the laser's trajectory create a sense of anticipation, as if Tom is about to perform an experiment that could lead to chaos or discovery. The background is filled with various scientific equipment, adding to the sense of a well-used and active workspace. The overall scene is one of tension and curiosity, as viewers wait to see what Tom's next move will be.'\n",
      "1: 'In the heart of a bustling city, a lone figure stands on a rooftop, silhouetted against the setting sun. Clad in a trench coat and fedora, the mysterious individual gazes out over the sprawling metropolis, lost in thought. The warm hues of the sunset paint the sky in shades of orange and pink, casting long shadows across the cityscape. In the distance, the towering skyscrapers stand as silent sentinels, their glass facades reflecting the fading light. The figure's posture is one of quiet contemplation, adding an air of intrigue to the scene. This moment, frozen in time, captures the essence of urban solitude and the beauty of a city at dusk.'\n",
      "6.0: 'In the heart of a cozy room bathed in warm hues, a mischievous gray cat lounges comfortably on a plush green cushion. Seated on a vibrant yellow pillow, the feline's relaxed posture exudes a sense of contentment. In its paws, it cradles a small, gleaming silver box, the object of its curiosity. The room's rustic charm is accentuated by a wooden table nearby, upon which rests a solitary candle, casting a soft glow that dances on the cat's fur. The scene is a harmonious blend of tranquility and intrigue, inviting the viewer to ponder the story behind the silver box and the cat's intent gaze.'\n",
      "\n",
      "\n",
      "You should output your summary directly, and not mention\n",
      "variables like ‘image_captions‘ in your response.\n",
      "Do not include ‘\\n’ and the word ’video’ in your response.\n",
      "Do not use introductory phrases such as: \"The video presents\", \"The video depicts\", \"This video showcases\",\n",
      "\"The video captures\" and so on.\n",
      " Please start the description with the video content directly, such as \"A man\n",
      "first sits in a chair, then stands up and walks to the kitchen....\"\n",
      " Do not use phrases like: \"as the video\n",
      "progressed\" and \"Throughout the video\".\n",
      " Please describe  the content of the video and the changes that occur, in\n",
      "chronological order.\n",
      " Please keep the description of this video within 100 English words.\n",
      "caption_summary_text:In a dimly lit laboratory, Tom the brown cat, wearing red gloves, wields a green laser pointer, targeting a beaker filled with red liquid amidst scientific equipment. The scene shifts to a rooftop at sunset, where a figure in a trench coat and fedora contemplates the cityscape, skyscrapers reflecting the fading light. Later, in a cozy room, a gray cat relaxes on a green cushion, holding a silver box, as a candle on a wooden table casts a soft glow. The transitions from a tense experiment to urban solitude and finally to feline tranquility create a narrative of curiosity and contemplation.\n"
     ]
    }
   ],
   "source": [
    "split_video_with_chunk('/mnt/ceph/develop/jiawei/lora_dataset/speech_data/猫和老鼠_只有人物/','test_150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1ab722d-79ad-40f8-bead-9329982e33c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_542.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_542.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_697.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_697.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1268.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1268.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1268.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1139.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1139.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1139.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1125.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1125.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1125.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_984.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_984.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_984.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_984.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_924.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_924.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_924.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_924.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_5.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_5.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([72, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_11//scene_chunks/videos/chunk_0.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([72, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_11//scene_chunks/videos/chunk_0.0_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_253.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_253.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_253.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_253.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_279.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_279.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_364.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_364.0_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([85, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_14//scene_chunks/videos/chunk_155.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([85, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_14//scene_chunks/videos/chunk_155.0_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_15//scene_chunks/videos/chunk_97.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_15//scene_chunks/videos/chunk_97.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_15//scene_chunks/videos/chunk_187.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_15//scene_chunks/videos/chunk_187.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_16//scene_chunks/videos/chunk_0.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_16//scene_chunks/videos/chunk_0.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_16//scene_chunks/videos/chunk_0.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_16//scene_chunks/videos/chunk_0.0_4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/labels\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos/chunk_77.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos/chunk_77.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos/chunk_100.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos/chunk_100.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/videos/chunk_191.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/videos/chunk_191.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([76, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos/chunk_141.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([76, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos/chunk_141.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([76, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos/chunk_141.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/videos/chunk_407.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/videos/chunk_407.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos/chunk_27.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos/chunk_27.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos/chunk_27.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos/chunk_27.0_4.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos/chunk_224.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos/chunk_224.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos/chunk_224.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_144.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_144.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_91.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_91.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_222.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_222.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_53.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_53.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_243.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_243.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_243.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_243.0_4.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_264.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_264.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_264.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_187.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_187.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_187.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_202.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_202.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_202.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_29//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_29//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_29//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_29//scene_chunks/labels\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/videos/chunk_188.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/videos/chunk_188.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_4//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_4//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_4//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_4//scene_chunks/labels\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_5//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_5//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_5//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_5//scene_chunks/labels\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/videos/chunk_567.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/videos/chunk_567.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_18.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_18.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_18.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_18.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_4.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos/chunk_161.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos/chunk_161.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos/chunk_161.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_9//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_9//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_9//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_9//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = '/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站/'\n",
    "for root, dirs, files in os.walk(root_path):\n",
    "    # 如果你只想获取下一层的子目录，可以在这里筛选\n",
    "    if root == root_path:\n",
    "        # root_dir 下的直接子目录就是 dirs 中的项\n",
    "        for dir in dirs:\n",
    "            split_video_with_chunk(root_path,dir)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
