{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ffa71c-27ab-4cc1-b075-134c365576b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 02:17:37,613 - modelscope - INFO - PyTorch version 2.0.1 Found.\n",
      "2023-09-11 02:17:37,615 - modelscope - INFO - TensorFlow version 2.10.0 Found.\n",
      "2023-09-11 02:17:37,615 - modelscope - INFO - Loading ast index from /home/dmeck/.cache/modelscope/ast_indexer\n",
      "2023-09-11 02:17:37,616 - modelscope - INFO - No valid ast index found from /home/dmeck/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2023-09-11 02:17:37,719 - modelscope - INFO - Loading done! Current index file version is 1.9.0, with md5 e74c3ad0d32f5b2c017304f8c2d8a5f8 and a total number of 921 components indexed\n",
      "2023-09-11 02:17:38,019 - modelscope - INFO - Model revision not specified, use the latest revision: v1.0.1\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 4.87k/4.87k [00:00<00:00, 490kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 126k/126k [00:00<00:00, 336kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1.11M/1.11M [00:00<00:00, 1.99MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 587k/587k [00:00<00:00, 862kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 65.2kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 597k/597k [00:00<00:00, 1.22MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 271k/271k [00:00<00:00, 1.65MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 170k/170k [00:00<00:00, 657kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 383k/383k [00:00<00:00, 992kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 6.07k/6.07k [00:00<00:00, 1.72MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1.52G/1.52G [02:32<00:00, 10.7MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10.3k/10.3k [00:00<00:00, 15.1MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 394/394 [00:00<00:00, 625kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:00<00:00, 68.7kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 107k/107k [00:00<00:00, 1.01MB/s]\n"
     ]
    }
   ],
   "source": [
    "#from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "#model_dir = snapshot_download('damo/multi-modal_clip-vit-large-patch14_336_zh',\n",
    "#                              cache_dir='/media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh',\n",
    "#                              revision='v1.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66de15ca-27b6-4fda-a331-598326888a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 03:47:51,854 - modelscope - INFO - initiate model from /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh\n",
      "2023-09-11 03:47:51,854 - modelscope - INFO - initiate model from location /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh.\n",
      "2023-09-11 03:47:51,855 - modelscope - INFO - initialize model from /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh\n",
      "2023-09-11 03:47:51,856 - modelscope - INFO - Loading vision model config from /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh/vision_model_config.json\n",
      "2023-09-11 03:47:51,856 - modelscope - INFO - Loading text model config from /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh/text_model_config.json\n",
      "2023-09-11 03:47:54,437 - modelscope - INFO - Use GPU 0 for finetuning & inference\n",
      "2023-09-11 03:47:54,446 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-09-11 03:47:54,446 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-09-11 03:47:54,447 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh'}. trying to build by task and model information.\n",
      "2023-09-11 03:47:54,447 - modelscope - WARNING - No preprocessor key ('clip-multi-modal-embedding', 'multi-modal-embedding') found in PREPROCESSOR_MAP, skip building preprocessor.\n"
     ]
    }
   ],
   "source": [
    "# require modelscope>=0.3.7，目前默认已经超过，您检查一下即可\n",
    "# 按照更新镜像的方法处理或者下面的方法\n",
    "# pip install --upgrade modelscope -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n",
    "# 需要单独安装decord，安装方法：pip install decord\n",
    "import torch\n",
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.preprocessors.image import load_image\n",
    "\n",
    "pipeline = pipeline(task=Tasks.multi_modal_embedding,\n",
    "    model='/media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh', model_revision='v1.0.1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeed44da-cf3b-4eac-9897-d6cbec563fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图文匹配概率: [[0.007717 0.2637   0.717    0.01177 ]]\n"
     ]
    }
   ],
   "source": [
    "input_img = load_image('/home/dmeck/Downloads/兔兔没有牙/0.jpg') # 支持皮卡丘示例图片路径/本地图片 返回PIL.Image\n",
    "input_texts = [\"女人\", \"拍照\", \"镜子\", \"横幅\"]\n",
    "# 支持一张图片(PIL.Image)或多张图片(List[PIL.Image])输入，输出归一化特征向量\n",
    "img_embedding = pipeline.forward({'img': input_img})['img_embedding'] # 2D Tensor, [图片数, 特征维度]\n",
    "\n",
    "# 支持一条文本(str)或多条文本(List[str])输入，输出归一化特征向量\n",
    "text_embedding = pipeline.forward({'text': input_texts})['text_embedding'] # 2D Tensor, [文本数, 特征维度]\n",
    "\n",
    "# 计算图文相似度\n",
    "with torch.no_grad():\n",
    "    # 计算内积得到logit，考虑模型temperature\n",
    "    logits_per_image = (img_embedding / pipeline.model.temperature) @ text_embedding.t()\n",
    "    # 根据logit计算概率分布\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"图文匹配概率:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c37e9bd-e04a-4b04-a881-26f3e92e5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "'''\n",
    "获取输入文件夹内的所有jpg文件，并返回文件名全称列表\n",
    "'''\n",
    "def load_jpg(file_dir):\n",
    "    L = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == '.jpg':\n",
    "                filename=os.path.join(root, file)\n",
    "                L.append(filename)\n",
    "        return L \n",
    "        \n",
    "def batch(iterable, size):\n",
    "    # range对象的step是size\n",
    "    for i in range(0, len(iterable), size):\n",
    "        yield iterable[i:i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc53525d-0ee9-4c0c-ba0a-b37a9e9c2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取数据，成功1599\n"
     ]
    }
   ],
   "source": [
    "data_folder = '/home/dmeck/Downloads/兔兔没有牙-features'\n",
    "\n",
    "jpg_files=load_jpg(data_folder) \n",
    "print(\"获取数据，成功{}\".format(len(jpg_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "981adb8e-7b19-49c9-95cd-816c05732bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▊                                                                                                            | 54/1599 [00:03<01:30, 17.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1596/1599 [00:33<00:00, 48.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 768)\n",
      "(1599, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1599/1599 [00:52<00:00, 48.69it/s]"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm  # Import the tqdm function or class\n",
    "import numpy as np\n",
    " \n",
    "#对每一个文件进行操作\n",
    "batch_len=3\n",
    "batch_list = batch(jpg_files,batch_len)\n",
    "b_unit = tqdm(enumerate(jpg_files),total=len(jpg_files))\n",
    "\n",
    "img_embedding_list = []\n",
    "\n",
    "for batch_file in batch_list:  \n",
    "    table_data=[]\n",
    "    for filename in batch_file:\n",
    "        \n",
    "        input_img = load_image(filename)\n",
    "        # 支持一张图片(PIL.Image)或多张图片(List[PIL.Image])输入，输出归一化特征向量\n",
    "        # 2D Tensor, [图片数, 特征维度]\n",
    "        img_embedding = pipeline.forward({'img': input_img})['img_embedding']\n",
    "        #torch.Size([1, 768])形状转换 torch.Size([768])\n",
    "        tensor_squeezed = img_embedding.squeeze()\n",
    "        img_embedding_list.append(tensor_squeezed)\n",
    "    # 更新进度\n",
    "    b_unit.update(batch_len) \n",
    "\n",
    "\n",
    "# 循环结束后关闭进度条\n",
    "b_unit.close()\n",
    "\n",
    "save_name = os.path.join(data_folder, f\"features.pkl\")\n",
    "# Open the file in write mode\n",
    "with open(save_name, 'wb') as file:\n",
    "    \n",
    "    features = [tensor.cpu() for tensor in img_embedding_list]\n",
    "    features_np = [tensor.numpy() for tensor in features] \n",
    "    print(np.asarray(features_np).shape)\n",
    "    pickle.dump(features_np, file)\n",
    "    file.close\n",
    " \n",
    "\n",
    "\n",
    "# Read inference data from local\n",
    "with open(save_name, 'rb') as file:\n",
    "    features = pickle.load(file)\n",
    "\n",
    "\n",
    "features = np.asarray(features)\n",
    "\n",
    "print(features.shape)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3d049-3e3b-42b1-b8a1-eff74d5022da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
