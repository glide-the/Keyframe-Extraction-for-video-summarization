{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56d48f9-3c03-4c6e-b4e1-6820f00567a2",
   "metadata": {},
   "source": [
    "#### 视频裁剪增加镜头描述\n",
    "对视频分片为，首帧和尾帧的描述，总结之后作为训练提示词\n",
    "注意调整split_video的默认参数，用于处理横屏数据w=540 , h=360或者竖屏数据 w=720 , h=960， \n",
    "这个参数按照原视频大小填写\n",
    "\n",
    "\n",
    "提示词镜头描述使用了glm-4-9b模型，你可以使用在线的平台，也可以单独在本地加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e606c5f-f004-4f6f-b64e-a093ef771c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a183c2a-dd46-4e12-88c5-b47fa61d4eda",
   "metadata": {},
   "source": [
    "### 安装环境后需要重启虚拟机核心"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3fc4293-63f4-465b-836d-61022b503db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install Pillow==10.1.0 torch==2.1.2 torchvision==0.16.2 transformers==4.40.0 sentencepiece==0.1.99 accelerate==0.30.1 bitsandbytes==0.43.1 peft==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d81b07a-8ff4-4481-b8d7-8e9e98cc53b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/conda_env/keyframe_extra/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-19 15:22:20.695522: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 15:22:21.469016: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-19 15:22:21.559133: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/ceph/develop/jiawei/conda_env/keyframe_extra/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2024-09-19 15:22:21.559162: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-09-19 15:22:21.688464: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-19 15:22:26.962652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/ceph/develop/jiawei/conda_env/keyframe_extra/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2024-09-19 15:22:26.963797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/ceph/develop/jiawei/conda_env/keyframe_extra/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2024-09-19 15:22:26.963804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/mnt/ceph/develop/jiawei/conda_env/keyframe_extra/lib/python3.10/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards:   0%|                                                                                                                                                     | 0/2 [00:00<?, ?it/s]/mnt/ceph/develop/jiawei/conda_env/keyframe_extra/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.62s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "model_path = '/mnt/ceph/develop/jiawei/ComfyUI/models/LLM/MiniCPMv2_6-prompt-generator'\n",
    "attention = 'sdpa'\n",
    "precision = 'fp16'\n",
    "dtype = {\"bf16\": torch.bfloat16, \"fp16\": torch.float16, \"fp32\": torch.float32}[precision]\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, attn_implementation=attention,\n",
    "                                                         torch_dtype=dtype, load_in_4bit=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6453b16-5ebd-438b-b9b8-c8552c98d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from typing import Union, List\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "import PIL.Image\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "\n",
    "def pad_frame(img, scale):\n",
    "    _, _, h, w = img.shape\n",
    "    tmp = max(32, int(32 / scale))\n",
    "    ph = ((h - 1) // tmp + 1) * tmp\n",
    "    pw = ((w - 1) // tmp + 1) * tmp\n",
    "    padding = (0,  pw - w, 0, ph - h)\n",
    "    return F.pad(img, padding)\n",
    "\n",
    "def pad_video_of_np(samples, w, h):\n",
    "    print(f\"samples dtype:{samples.dtype}\")\n",
    "    print(f\"samples shape:{samples.shape}\")\n",
    "    output = []\n",
    "    # [f, c, h, w]\n",
    "    for b in range(samples.shape[0]):\n",
    "        frame = samples[b : b + 1]\n",
    "        frame = pad_frame(frame, 1).to(dtype=samples.dtype)\n",
    "        \n",
    "        frame = F.interpolate(frame, size=(h, w))\n",
    "        output.append(frame.squeeze(0)) # (to [f, w, h, c])\n",
    "\n",
    "    image_np = VaeImageProcessor.pt_to_numpy(torch.stack(output))  # (to [49, 512, 480, 3])\n",
    "    image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "    return image_pil\n",
    "\n",
    "\n",
    "def split_video(video_path, output_dir, start_time, end_time, features_frame, w=540 , h=360 ,chunk_duration=3):\n",
    "    # Open the video file\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)  # Get the frames per second\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))  # Total frames in the video\n",
    "    output_files = []\n",
    "    # Calculate start and end frames based on time\n",
    "    start_frame = int(start_time * fps)\n",
    "    end_frame = int(end_time * fps)\n",
    "\n",
    "    video_capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)  # Set video position to start frame\n",
    "\n",
    "    current_frame = start_frame\n",
    "    chunk_index = 0\n",
    "\n",
    "    while current_frame <= end_frame:\n",
    "        chunk_frames = []\n",
    "        for _ in range(int(chunk_duration * fps)):\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break  \n",
    "            frame_rgb = frame[..., ::-1]\n",
    "            \n",
    "            # 创建一个新数组，确保 stride 是正的\n",
    "            frame_rgb = frame_rgb.copy()\n",
    "            tensor = torch.from_numpy(frame_rgb).float().to(\"cpu\", non_blocking=True).float() / 255.0\n",
    "            chunk_frames.append(\n",
    "                tensor.permute(2, 0, 1)\n",
    "            )  # to [c, h, w,]\n",
    "            current_frame += 1\n",
    "\n",
    "        if len(chunk_frames) == 0:\n",
    "            break\n",
    "            \n",
    "        # If the last chunk is shorter than expected, repeat the last frame\n",
    "        while len(chunk_frames) < int(chunk_duration * fps):\n",
    "            chunk_frames.append(chunk_frames[-1])\n",
    "            \n",
    "        pt_frame = torch.from_numpy(np.stack(chunk_frames))  # to [f, c, h, w]\n",
    "         \n",
    "        chunk_images = pad_video_of_np(pt_frame,w,h)\n",
    "     \n",
    "        # Save the chunk as a video\n",
    "        output_file = os.path.join(output_dir, f'chunk_{features_frame}_{chunk_index}.mp4')\n",
    "        print(output_file)\n",
    "        save_chunk_as_video(chunk_images, output_file, fps=math.ceil((len(chunk_images) - 1) / 6))\n",
    "        output_files.append(output_file)\n",
    "        chunk_index += 1\n",
    "\n",
    "    video_capture.release()\n",
    "    return output_files\n",
    "    \n",
    " \n",
    "    \n",
    "def save_chunk_as_video(tensor: Union[List[np.ndarray], List[PIL.Image.Image]],video_path, fps: int = 8):\n",
    "    os.makedirs(os.path.dirname(video_path), exist_ok=True)\n",
    "    export_to_video(tensor, video_path, fps=fps)\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57626e8c-6d29-434c-bb60-b1cadb9450ab",
   "metadata": {},
   "source": [
    "#### 使用gpt重新给这个场景增加描述\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce58a0f-7a25-4b45-b079-46778743cc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65afd641-6049-4597-a652-9e08dd5c9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"9ba20fe4907877f510c193ecfd810b29.qakNNGbHWdF7dzgs\",\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    import base64\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "def caption_chunk_gpt(video_path, features_caption):\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    ret, first_frame = video_capture.read()\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_capture.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)\n",
    "    ret, last_frame = video_capture.read()\n",
    "\n",
    "    # Get the frame rate (frames per second)\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the duration of the video in seconds\n",
    "    video_duration = total_frames / fps\n",
    "    \n",
    "    video_capture.release()\n",
    "    \n",
    "    first_frame_rgb = first_frame[..., ::-1] \n",
    "    first_frame_rgb = first_frame_rgb.copy()\n",
    "\n",
    "    last_frame_rgb = last_frame[..., ::-1] \n",
    "    last_frame_rgb = last_frame_rgb.copy()\n",
    "    \n",
    "    first_frame_pil_image = Image.fromarray(first_frame_rgb)\n",
    "\n",
    "    last_frame_pil_image = Image.fromarray(last_frame_rgb)\n",
    "\n",
    "    prompt = \"\"\"Follow these steps to create a Midjourney-style long prompt for generating high-quality images: \n",
    "            1. The prompt should include rich details, vivid scenes, and composition information, capturing the important elements that make up the scene. \n",
    "            2. You can appropriately add some details to enhance the vividness and richness of the content, while ensuring that the long prompt does not exceed 256 tokens,you should only return prompt，itself without any additional information\"\"\"\n",
    "\n",
    "    # Prepare the input for the chat method\n",
    "    msgs = [{\"role\": \"user\", \"content\": [first_frame_pil_image, prompt]}]\n",
    "    # Use the chat method\n",
    "    first_frame_generated_text = model.chat(\n",
    "        image=[first_frame_pil_image],\n",
    "        msgs=msgs,\n",
    "        tokenizer=tokenizer,\n",
    "        processor=processor,\n",
    "        max_new_tokens=2048,\n",
    "        sampling=False,\n",
    "        num_beams=3\n",
    "    ) \n",
    "    \n",
    "    # Prepare the input for the chat method\n",
    "    msgs = [{\"role\": \"user\", \"content\": [last_frame_pil_image, prompt]}]\n",
    "    # Use the chat method\n",
    "    last_frame_generated_text = model.chat(\n",
    "        image=[last_frame_pil_image],\n",
    "        msgs=msgs,\n",
    "        tokenizer=tokenizer,\n",
    "        processor=processor,\n",
    "        max_new_tokens=2048,\n",
    "        sampling=False,\n",
    "        num_beams=3\n",
    "    ) \n",
    "    # Sample dictionary of image captions (this should be generated based on your video analysis)\n",
    "    image_captions = { \n",
    "        \"1\": first_frame_generated_text,\n",
    "        video_duration: last_frame_generated_text\n",
    "    }\n",
    "    \n",
    "    # Convert image_captions dictionary into a format suitable for new_captions\n",
    "    new_captions = \"\\n\".join([f\"{time}: '{description}'\" for time, description in image_captions.items()])\n",
    "\n",
    "    caption_summary_prompt = f\"\"\"Video description: \\\"{features_caption}\\\"\\n\\n\n",
    "We extracted several frames from this video and described\n",
    "each frame using an image understanding model, stored in the dictionary variable ‘image_captions: Dict[str: str]‘.\n",
    "In ‘image_captions‘, the key is the second at which the image appears in the video, and the value is a detailed description\n",
    "of the image at that moment. Please describe the content of this video in as much detail as possible, based on the\n",
    "information provided by ‘image_captions‘, including the objects, scenery, animals, characters, and camera\n",
    "movements within the video. \\n image_captions={new_captions}\\n\\n\n",
    "You should output your summary directly, and not mention\n",
    "variables like ‘image_captions‘ in your response.\n",
    "Do not include ‘\\\\n’ and the word ’video’ in your response.\n",
    "Do not use introductory phrases such as: \\\"The video presents\\\", \\\"The video depicts\\\", \\\"This video showcases\\\",\n",
    "\\\"The video captures\\\" and so on.\\n Please start the description with the video content directly, such as \\\"A man\n",
    "first sits in a chair, then stands up and walks to the kitchen....\\\"\\n Do not use phrases like: \\\"as the video\n",
    "progressed\\\" and \\\"Throughout the video\\\".\\n Please describe  the content of the video and the changes that occur, in\n",
    "chronological order.\\n Please keep the description of this video within 100 English words.\"\"\"\n",
    "\n",
    "    print(f\"caption_summary_prompt:{caption_summary_prompt}\")\n",
    "    # 减少信息，生成速度可以在0.1秒完成\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"web_search\",\n",
    "            \"web_search\": {\n",
    "                \"enable\": False, \n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4-plus\",\n",
    "        messages=[ {\"role\": \"user\", \"content\": f\"{caption_summary_prompt}\"}],\n",
    "        temperature=0,\n",
    "        tools=tools,\n",
    "        max_tokens=2000,\n",
    "    ) \n",
    "\n",
    "    caption_summary_text = response.choices[0].message.content\n",
    "    print(f\"{video_path}\\r\\ncaption_summary_text:{caption_summary_text}\")\n",
    "    return first_frame_generated_text, last_frame_generated_text, caption_summary_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01901b-2ce9-4287-88b2-f0e13398ad87",
   "metadata": {},
   "source": [
    "#### 删除前后时间场景的误差时间 ， 按照3秒每个片段裁剪, fps是原视频的\n",
    "获取第一帧和最后一帧画面，使用gpt整理视频提示词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da84ff3b-15c6-4216-8fb0-fcdf86442219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loadcsv_scene_chunk(scene_csv_path, video_path, videos_output_dir, labels_output_dir):\n",
    "    df = pd.read_csv(scene_csv_path)\n",
    "    \n",
    "    # 计算每个分镜编号的总持续时间\n",
    "    df['持续时间'] = (pd.to_datetime(df['结束时间']) - pd.to_datetime(df['开始时间'])).dt.total_seconds()\n",
    "    grouped_df = df.groupby('分镜')['持续时间'].sum().reset_index()\n",
    "    df = df[df['持续时间'] >= 3]\n",
    "    \n",
    "    df = df.sort_values(by='持续时间', ascending=True)\n",
    "        \n",
    "    # 创建一个新的 DataFrame 来存储行\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    # 为新数据添加列\n",
    "    new_df['chunk_index'] = None\n",
    "    new_df['chunk_path'] = None\n",
    "    new_df['first_frame_generated_text'] = None\n",
    "    new_df['last_frame_generated_text'] = None\n",
    "    new_df['caption_summary_text'] = None\n",
    " \n",
    "    new_df = pd.DataFrame()  # 确保 new_df 已被初始化\n",
    "\n",
    "    for _, row in df.iterrows(): \n",
    " \n",
    "        start_time = row['开始时间']\n",
    "        end_time = row['结束时间']\n",
    "        features_caption = row['特征描述']\n",
    "        features_frame = int(row['特征帧']) \n",
    " \n",
    "        # 将开始时间和结束时间解析为时间对象\n",
    "        start_time_obj = datetime.datetime.strptime(start_time, \"%H:%M:%S,%f\")\n",
    "        end_time_obj = datetime.datetime.strptime(end_time, \"%H:%M:%S,%f\")\n",
    "    \n",
    "        # 计算时间区间的秒数\n",
    "        start_seconds = (start_time_obj.hour * 3600 + start_time_obj.minute * 60 + start_time_obj.second +\n",
    "                         start_time_obj.microsecond / 1000000)\n",
    "        end_seconds = (end_time_obj.hour * 3600 + end_time_obj.minute * 60 + end_time_obj.second +\n",
    "                       end_time_obj.microsecond / 1000000)\n",
    "        start_seconds = start_seconds-1\n",
    "        end_seconds = end_seconds-1\n",
    "        # Split the video for each start and end time\n",
    "        output_files = split_video(video_path, videos_output_dir, start_seconds, end_seconds,features_frame)\n",
    "        print(f\"output_files:{output_files}\")\n",
    "        for index, path in enumerate(output_files):\n",
    "            # 在内层循环中，复制当前行的数据\n",
    "            new_row = row.copy()\n",
    "            first_frame_generated_text, last_frame_generated_text, caption_summary_text = caption_chunk_gpt(path, features_caption)\n",
    "             \n",
    "            # 更新新行的数据\n",
    "            new_row['chunk_index'] = f'chunk_{features_frame}_{index}'\n",
    "            new_row['chunk_path'] = path\n",
    "            new_row['first_frame_generated_text'] = first_frame_generated_text\n",
    "            new_row['last_frame_generated_text'] = last_frame_generated_text\n",
    "            new_row['caption_summary_text'] = caption_summary_text\n",
    "            \n",
    "            # 将新行转换为 DataFrame 并追加到 new_df\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            lable_file = os.path.join(labels_output_dir, f'chunk_{features_frame}_{index}.txt')\n",
    "            # Open the file in write mode\n",
    "            with open(lable_file, 'w') as file: \n",
    "                file.write(caption_summary_text)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d713d948-cf22-47dc-ad74-933066d459b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_output_folder(output_folder):\n",
    "    if not os.path.isdir(output_folder):\n",
    "        print('warning: the folder{} is not exist'.format(output_folder))\n",
    "        # create srt_folder\n",
    "        os.makedirs(output_folder)\n",
    "        print('create folder', output_folder)\n",
    "\n",
    "def split_video_with_chunk(root_path, video_source):\n",
    "    save_path = f'{root_path}/{video_source}/'\n",
    "    video_path=f'{save_path}/{video_source}.mp4' \n",
    " \n",
    "    scene_csv_path=f'{save_path}/scene/{video_source}_scene_keyframe.csv'\n",
    "    videos_output_dir = f'{save_path}/scene_chunks/videos'\n",
    "    labels_output_dir = f'{save_path}/scene_chunks/labels'\n",
    "\n",
    "    scene_chunks_csv_path = f'{save_path}/scene_chunks/{video_source}_scene_chunk_frame.csv'\n",
    "    check_output_folder(videos_output_dir)\n",
    "\n",
    "    check_output_folder(labels_output_dir)\n",
    "    \n",
    "    new_df = loadcsv_scene_chunk(scene_csv_path,video_path,videos_output_dir, labels_output_dir)\n",
    "    \n",
    "    new_df.to_csv(scene_chunks_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec3b902e-42ae-4c57-bc11-2978778e9bd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3335373/2572951941.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['持续时间'] = (pd.to_datetime(df['结束时间']) - pd.to_datetime(df['开始时间'])).dt.total_seconds()\n",
      "/tmp/ipykernel_3335373/2572951941.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['持续时间'] = (pd.to_datetime(df['结束时间']) - pd.to_datetime(df['开始时间'])).dt.total_seconds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (540, 360) to (544, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_542_0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[swscaler @ 0x590b180] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (540, 360) to (544, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_542_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[swscaler @ 0x6c52180] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_files:['/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_542_0.mp4', '/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_542_1.mp4']\n",
      "caption_summary_prompt:Video description: \"As the neon lights flicker along the dimly lit corridor, a group of friends strolls hand in hand, their laughter echoing through the space. The walls are adorned with vibrant posters, each telling a story of its own. The atmosphere is one of anticipation and excitement, as if they are about to embark on an adventure. The soft glow of the lights casts a warm hue over the scene, highlighting the joy and camaraderie among the group. The text above them reads, \"晚上，奇奇猪终于来啦！我们一起去吃饭~\" which translates to \"Tonight, Qiqi Pig finally arrives Let's go eat together~\"\"\n",
      "\n",
      "\n",
      "We extracted several frames from this video and described\n",
      "each frame using an image understanding model, stored in the dictionary variable ‘image_captions: Dict[str: str]‘.\n",
      "In ‘image_captions‘, the key is the second at which the image appears in the video, and the value is a detailed description\n",
      "of the image at that moment. Please describe the content of this video in as much detail as possible, based on the\n",
      "information provided by ‘image_captions‘, including the objects, scenery, animals, characters, and camera\n",
      "movements within the video. \n",
      " image_captions=1: 'In the dimly lit interior of a modern art gallery, two young women stand closely together, their backs to the viewer. The woman on the left, donning a cozy purple hoodie and a whimsical hat adorned with bunny ears, leans slightly towards her companion. Her companion, with long, flowing hair cascading down her back, is dressed in a chic black and white ensemble. The gallery walls are adorned with vibrant, abstract paintings, their colors popping against the subdued lighting. The atmosphere is one of quiet anticipation, as if the women are about to embark on a journey through the realms of art and imagination.'\n",
      "5.916666666666667: 'A long prompt for generating a high-quality image might look like this:\n",
      "\n",
      "In the dimly lit room, a solitary figure sits on the edge of a plush purple couch, their silhouette barely discernible against the soft glow of a nearby lamp. The atmosphere is one of quiet contemplation, the only sounds being the gentle hum of the room's air conditioning and the occasional rustle of papers on the coffee table. The figure's hands are clasped together, resting on their lap, as if in deep thought or perhaps waiting for someone to join them. The walls are adorned with abstract art, their colors muted in the low light, adding to the overall sense of calm and introspection. The scene is captured from a low angle, looking up at the figure, which adds a sense of intimacy and immediacy to the moment.'\n",
      "\n",
      "\n",
      "You should output your summary directly, and not mention\n",
      "variables like ‘image_captions‘ in your response.\n",
      "Do not include ‘\\n’ and the word ’video’ in your response.\n",
      "Do not use introductory phrases such as: \"The video presents\", \"The video depicts\", \"This video showcases\",\n",
      "\"The video captures\" and so on.\n",
      " Please start the description with the video content directly, such as \"A man\n",
      "first sits in a chair, then stands up and walks to the kitchen....\"\n",
      " Do not use phrases like: \"as the video\n",
      "progressed\" and \"Throughout the video\".\n",
      " Please describe  the content of the video and the changes that occur, in\n",
      "chronological order.\n",
      " Please keep the description of this video within 100 English words.\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_542_0.mp4\n",
      "caption_summary_text:In a dimly lit modern art gallery, two young women stand close, backs to the viewer. One wears a purple hoodie and a hat with bunny ears, the other a chic black and white outfit. Vibrant abstract paintings adorn the walls. At 5.9 seconds, a solitary figure sits on a plush purple couch in a softly lit room, hands clasped in contemplation. Abstract art on the walls adds to the calm. The scene shifts from the anticipation of the gallery to the introspective solitude of the couch, capturing moments of quiet and thought. The atmosphere transitions from communal excitement to personal reflection.\n",
      "caption_summary_prompt:Video description: \"As the neon lights flicker along the dimly lit corridor, a group of friends strolls hand in hand, their laughter echoing through the space. The walls are adorned with vibrant posters, each telling a story of its own. The atmosphere is one of anticipation and excitement, as if they are about to embark on an adventure. The soft glow of the lights casts a warm hue over the scene, highlighting the joy and camaraderie among the group. The text above them reads, \"晚上，奇奇猪终于来啦！我们一起去吃饭~\" which translates to \"Tonight, Qiqi Pig finally arrives Let's go eat together~\"\"\n",
      "\n",
      "\n",
      "We extracted several frames from this video and described\n",
      "each frame using an image understanding model, stored in the dictionary variable ‘image_captions: Dict[str: str]‘.\n",
      "In ‘image_captions‘, the key is the second at which the image appears in the video, and the value is a detailed description\n",
      "of the image at that moment. Please describe the content of this video in as much detail as possible, based on the\n",
      "information provided by ‘image_captions‘, including the objects, scenery, animals, characters, and camera\n",
      "movements within the video. \n",
      " image_captions=1: 'In the dimly lit room, a sense of anticipation fills the air as the soft glow of purple lights casts an ethereal hue over the surroundings. Amidst the shadows, a figure emerges, their silhouette barely discernible in the low light. The atmosphere is charged with excitement, as if something momentous is about to unfold. In the foreground, a handwritten note catches the eye, its message hinting at a significant event or revelation. The overall mood is one of mystery and intrigue, inviting the viewer to step into this enigmatic setting and unravel its secrets.'\n",
      "5.916666666666667: 'In the heart of a bustling restaurant, a table is set with an array of vibrant and appetizing dishes. The centerpiece is a large rectangular tray filled with a rich, simmering red sauce, its steam rising gently into the air. Beside it, a white plate is adorned with neatly arranged cherry tomatoes, their bright red color contrasting beautifully against the plate's pristine surface. Scattered around the table are small, colorful food items, adding a playful touch to the dining experience. The background is softly blurred, focusing the viewer's attention on the delicious spread before them. The overall atmosphere is one of warmth and inviting culinary delight.'\n",
      "\n",
      "\n",
      "You should output your summary directly, and not mention\n",
      "variables like ‘image_captions‘ in your response.\n",
      "Do not include ‘\\n’ and the word ’video’ in your response.\n",
      "Do not use introductory phrases such as: \"The video presents\", \"The video depicts\", \"This video showcases\",\n",
      "\"The video captures\" and so on.\n",
      " Please start the description with the video content directly, such as \"A man\n",
      "first sits in a chair, then stands up and walks to the kitchen....\"\n",
      " Do not use phrases like: \"as the video\n",
      "progressed\" and \"Throughout the video\".\n",
      " Please describe  the content of the video and the changes that occur, in\n",
      "chronological order.\n",
      " Please keep the description of this video within 100 English words.\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_542_1.mp4\n",
      "caption_summary_text:In a dimly lit room, purple lights cast an ethereal glow, creating a sense of anticipation. A figure emerges from the shadows, barely visible. A handwritten note in the foreground hints at a significant event. The scene shifts to a bustling restaurant where a table is set with vibrant dishes. A large tray of simmering red sauce steams, beside a plate of cherry tomatoes. Small, colorful food items add playfulness. The background is softly blurred, focusing on the inviting culinary spread. The transition from mysterious anticipation to warm, delightful dining captures the essence of an exciting evening gathering.\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (540, 360) to (544, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_243_0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[swscaler @ 0x5f60180] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (540, 360) to (544, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_243_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[swscaler @ 0x57ba180] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_files:['/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_243_0.mp4', '/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_243_1.mp4']\n",
      "caption_summary_prompt:Video description: \"As the train glides smoothly along the tracks, the sunlight streams through the window, casting a warm glow on the interior. The view outside reveals a serene landscape, with gentle hills stretching into the distance and a clear blue sky overhead. The composition of the scene is balanced, with the train's window framing the natural beauty outside, creating a harmonious blend of man-made and natural elements. The atmosphere is peaceful, evoking a sense of calm and tranquility as the train continues its journey.\"\n",
      "\n",
      "\n",
      "We extracted several frames from this video and described\n",
      "each frame using an image understanding model, stored in the dictionary variable ‘image_captions: Dict[str: str]‘.\n",
      "In ‘image_captions‘, the key is the second at which the image appears in the video, and the value is a detailed description\n",
      "of the image at that moment. Please describe the content of this video in as much detail as possible, based on the\n",
      "information provided by ‘image_captions‘, including the objects, scenery, animals, characters, and camera\n",
      "movements within the video. \n",
      " image_captions=1: 'In the dimly lit streets of an ancient city, a lone figure navigates the cobblestone pathways, illuminated only by the soft glow of distant lanterns. The air is thick with the scent of incense, and the sound of distant bells echoes through the narrow alleyways. The figure, clad in traditional attire, carries a lantern, casting dancing shadows on the uneven ground. In the background, the silhouette of a majestic temple looms, its intricate carvings and towering spires reaching towards the heavens. The atmosphere is one of quiet contemplation and historical reverence, as if time itself has paused to admire the beauty of this timeless scene.'\n",
      "5.916666666666667: 'As the train glides smoothly along the tracks, a serene landscape unfolds outside the window. The sky is a canvas of soft hues, painted with gentle strokes of blue and pink, hinting at the break of dawn or the onset of dusk. Below, a vast expanse of water stretches out, its surface shimmering under the diffused light, reflecting the colors of the sky above. In the distance, the faint outline of land can be seen, adding depth to the tranquil scene. The window frame, a muted shade of gray, contrasts with the vibrant colors outside, framing this moment of peaceful travel. The atmosphere is one of calm and quiet, a perfect escape from the hustle and bustle of daily life.'\n",
      "\n",
      "\n",
      "You should output your summary directly, and not mention\n",
      "variables like ‘image_captions‘ in your response.\n",
      "Do not include ‘\\n’ and the word ’video’ in your response.\n",
      "Do not use introductory phrases such as: \"The video presents\", \"The video depicts\", \"This video showcases\",\n",
      "\"The video captures\" and so on.\n",
      " Please start the description with the video content directly, such as \"A man\n",
      "first sits in a chair, then stands up and walks to the kitchen....\"\n",
      " Do not use phrases like: \"as the video\n",
      "progressed\" and \"Throughout the video\".\n",
      " Please describe  the content of the video and the changes that occur, in\n",
      "chronological order.\n",
      " Please keep the description of this video within 100 English words.\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_243_0.mp4\n",
      "caption_summary_text:In the dimly lit streets of an ancient city, a lone figure in traditional attire navigates cobblestone pathways, illuminated by distant lanterns. The air is thick with incense, and distant bells echo through narrow alleyways. A majestic temple's silhouette looms in the background. At 5.9167 seconds, the scene shifts to a train gliding smoothly along tracks. Outside the window, a serene landscape unfolds with a sky painted in soft hues of blue and pink. A vast, shimmering water expanse reflects the sky, and a faint land outline adds depth. The window frame contrasts with the vibrant exterior, encapsulating a moment of peaceful travel.\n",
      "caption_summary_prompt:Video description: \"As the train glides smoothly along the tracks, the sunlight streams through the window, casting a warm glow on the interior. The view outside reveals a serene landscape, with gentle hills stretching into the distance and a clear blue sky overhead. The composition of the scene is balanced, with the train's window framing the natural beauty outside, creating a harmonious blend of man-made and natural elements. The atmosphere is peaceful, evoking a sense of calm and tranquility as the train continues its journey.\"\n",
      "\n",
      "\n",
      "We extracted several frames from this video and described\n",
      "each frame using an image understanding model, stored in the dictionary variable ‘image_captions: Dict[str: str]‘.\n",
      "In ‘image_captions‘, the key is the second at which the image appears in the video, and the value is a detailed description\n",
      "of the image at that moment. Please describe the content of this video in as much detail as possible, based on the\n",
      "information provided by ‘image_captions‘, including the objects, scenery, animals, characters, and camera\n",
      "movements within the video. \n",
      " image_captions=1: 'As the sun rises, casting a warm glow over the landscape, you find yourself seated by a window on a moving train. The view outside is a blur of motion, with the train tracks stretching out into the distance, disappearing into the horizon. The sky above is a canvas of soft hues, transitioning from the gentle pinks of dawn to the deeper blues of the early morning. The interior of the train is bathed in natural light, reflecting off the polished surfaces and creating a serene atmosphere. You can't help but feel a sense of calm and anticipation as you look out at the world passing by, ready to embrace the new day that lies ahead.'\n",
      "5.916666666666667: 'In the dimly lit interior of a train compartment, a young woman is captured in a moment of self-reflection. She stands in front of a small, rectangular mirror, her face partially obscured by the reflection. Dressed in a casual gray sweatshirt, her hair is tousled and unkempt, adding to the candid nature of the scene. The mirror reveals a glimpse of the train's interior, with its metallic surfaces and functional design. The ambient light casts soft shadows, highlighting the contours of her face and the texture of her hair. The overall atmosphere is one of quiet introspection, as if she is taking a moment to herself amidst her journey.'\n",
      "\n",
      "\n",
      "You should output your summary directly, and not mention\n",
      "variables like ‘image_captions‘ in your response.\n",
      "Do not include ‘\\n’ and the word ’video’ in your response.\n",
      "Do not use introductory phrases such as: \"The video presents\", \"The video depicts\", \"This video showcases\",\n",
      "\"The video captures\" and so on.\n",
      " Please start the description with the video content directly, such as \"A man\n",
      "first sits in a chair, then stands up and walks to the kitchen....\"\n",
      " Do not use phrases like: \"as the video\n",
      "progressed\" and \"Throughout the video\".\n",
      " Please describe  the content of the video and the changes that occur, in\n",
      "chronological order.\n",
      " Please keep the description of this video within 100 English words.\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test//test_1//scene_chunks/videos/chunk_243_1.mp4\n",
      "caption_summary_text:As the sun rises, casting a warm glow over the landscape, a passenger sits by a window on a moving train, watching the tracks stretch into the horizon. The sky transitions from soft pinks to deeper blues. Inside, the train is bathed in natural light, creating a serene atmosphere. At 5.9167 seconds, a young woman in a gray sweatshirt stands in front of a small mirror in a dimly lit compartment, her face partially obscured, lost in self-reflection. The mirror reflects the train's metallic interior, and soft shadows highlight her features, adding to the quiet, introspective mood amidst her journey.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/conda_env/keyframe_extra/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2540: RuntimeWarning: invalid value encountered in cast\n",
      "  values = values.astype(str)\n"
     ]
    }
   ],
   "source": [
    "split_video_with_chunk('/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test/','test_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10f5d89c-a2bf-497c-983d-ea93f41cc1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>空白</th>\n",
       "      <th>内容</th>\n",
       "      <th>开始时间</th>\n",
       "      <th>结束时间</th>\n",
       "      <th>分镜</th>\n",
       "      <th>特征帧</th>\n",
       "      <th>特征描述</th>\n",
       "      <th>持续时间</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_path</th>\n",
       "      <th>first_frame_generated_text</th>\n",
       "      <th>last_frame_generated_text</th>\n",
       "      <th>caption_summary_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00:00:22,652</td>\n",
       "      <td>00:00:25,695</td>\n",
       "      <td>12</td>\n",
       "      <td>542.0</td>\n",
       "      <td>As the neon lights flicker along the dimly lit...</td>\n",
       "      <td>3.043</td>\n",
       "      <td>chunk_542_0</td>\n",
       "      <td>/mnt/ceph/develop/jiawei/lora_dataset/speech_d...</td>\n",
       "      <td>In the dimly lit interior of a modern art gall...</td>\n",
       "      <td>A long prompt for generating a high-quality im...</td>\n",
       "      <td>In a dimly lit modern art gallery, two young w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00:00:22,652</td>\n",
       "      <td>00:00:25,695</td>\n",
       "      <td>12</td>\n",
       "      <td>542.0</td>\n",
       "      <td>As the neon lights flicker along the dimly lit...</td>\n",
       "      <td>3.043</td>\n",
       "      <td>chunk_542_1</td>\n",
       "      <td>/mnt/ceph/develop/jiawei/lora_dataset/speech_d...</td>\n",
       "      <td>In the dimly lit room, a sense of anticipation...</td>\n",
       "      <td>In the heart of a bustling restaurant, a table...</td>\n",
       "      <td>In a dimly lit room, purple lights cast an eth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00:00:08,608</td>\n",
       "      <td>00:00:11,739</td>\n",
       "      <td>5</td>\n",
       "      <td>243.0</td>\n",
       "      <td>As the train glides smoothly along the tracks,...</td>\n",
       "      <td>3.131</td>\n",
       "      <td>chunk_243_0</td>\n",
       "      <td>/mnt/ceph/develop/jiawei/lora_dataset/speech_d...</td>\n",
       "      <td>In the dimly lit streets of an ancient city, a...</td>\n",
       "      <td>As the train glides smoothly along the tracks,...</td>\n",
       "      <td>In the dimly lit streets of an ancient city, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00:00:08,608</td>\n",
       "      <td>00:00:11,739</td>\n",
       "      <td>5</td>\n",
       "      <td>243.0</td>\n",
       "      <td>As the train glides smoothly along the tracks,...</td>\n",
       "      <td>3.131</td>\n",
       "      <td>chunk_243_1</td>\n",
       "      <td>/mnt/ceph/develop/jiawei/lora_dataset/speech_d...</td>\n",
       "      <td>As the sun rises, casting a warm glow over the...</td>\n",
       "      <td>In the dimly lit interior of a train compartme...</td>\n",
       "      <td>As the sun rises, casting a warm glow over the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   空白  内容          开始时间          结束时间  分镜    特征帧  \\\n",
       "0 NaN NaN  00:00:22,652  00:00:25,695  12  542.0   \n",
       "1 NaN NaN  00:00:22,652  00:00:25,695  12  542.0   \n",
       "2 NaN NaN  00:00:08,608  00:00:11,739   5  243.0   \n",
       "3 NaN NaN  00:00:08,608  00:00:11,739   5  243.0   \n",
       "\n",
       "                                                特征描述   持续时间  chunk_index  \\\n",
       "0  As the neon lights flicker along the dimly lit...  3.043  chunk_542_0   \n",
       "1  As the neon lights flicker along the dimly lit...  3.043  chunk_542_1   \n",
       "2  As the train glides smoothly along the tracks,...  3.131  chunk_243_0   \n",
       "3  As the train glides smoothly along the tracks,...  3.131  chunk_243_1   \n",
       "\n",
       "                                          chunk_path  \\\n",
       "0  /mnt/ceph/develop/jiawei/lora_dataset/speech_d...   \n",
       "1  /mnt/ceph/develop/jiawei/lora_dataset/speech_d...   \n",
       "2  /mnt/ceph/develop/jiawei/lora_dataset/speech_d...   \n",
       "3  /mnt/ceph/develop/jiawei/lora_dataset/speech_d...   \n",
       "\n",
       "                          first_frame_generated_text  \\\n",
       "0  In the dimly lit interior of a modern art gall...   \n",
       "1  In the dimly lit room, a sense of anticipation...   \n",
       "2  In the dimly lit streets of an ancient city, a...   \n",
       "3  As the sun rises, casting a warm glow over the...   \n",
       "\n",
       "                           last_frame_generated_text  \\\n",
       "0  A long prompt for generating a high-quality im...   \n",
       "1  In the heart of a bustling restaurant, a table...   \n",
       "2  As the train glides smoothly along the tracks,...   \n",
       "3  In the dimly lit interior of a train compartme...   \n",
       "\n",
       "                                caption_summary_text  \n",
       "0  In a dimly lit modern art gallery, two young w...  \n",
       "1  In a dimly lit room, purple lights cast an eth...  \n",
       "2  In the dimly lit streets of an ancient city, a...  \n",
       "3  As the sun rises, casting a warm glow over the...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "scene_chunks_csv_path = f'/mnt/ceph/develop/jiawei/lora_dataset/speech_data/test/test_1/scene_chunks/test_1_scene_chunk_frame.csv'\n",
    "df = pd.read_csv(scene_chunks_csv_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1ab722d-79ad-40f8-bead-9329982e33c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_542.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_542.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_697.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_697.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1268.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1268.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1268.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1139.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1139.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1139.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1125.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1125.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1125.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1029.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_984.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_984.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_984.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_984.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_924.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_924.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_924.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_924.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_774.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_5.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1321.0_5.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([71, 3, 960, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_1//scene_chunks/videos/chunk_1511.0_5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([72, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_11//scene_chunks/videos/chunk_0.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([72, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_11//scene_chunks/videos/chunk_0.0_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_253.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_253.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_253.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_253.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_279.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_279.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_364.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_12//scene_chunks/videos/chunk_364.0_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([85, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_14//scene_chunks/videos/chunk_155.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([85, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_14//scene_chunks/videos/chunk_155.0_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_15//scene_chunks/videos/chunk_97.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_15//scene_chunks/videos/chunk_97.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_15//scene_chunks/videos/chunk_187.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_15//scene_chunks/videos/chunk_187.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_16//scene_chunks/videos/chunk_0.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_16//scene_chunks/videos/chunk_0.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_16//scene_chunks/videos/chunk_0.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_16//scene_chunks/videos/chunk_0.0_4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([82, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_17//scene_chunks/videos/chunk_143.0_4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/labels\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos/chunk_77.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos/chunk_77.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos/chunk_100.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_19//scene_chunks/videos/chunk_100.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_2//scene_chunks/videos/chunk_261.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/videos/chunk_191.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_20//scene_chunks/videos/chunk_191.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([76, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos/chunk_141.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([76, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos/chunk_141.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([76, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_21//scene_chunks/videos/chunk_141.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/videos/chunk_407.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_22//scene_chunks/videos/chunk_407.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos/chunk_27.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos/chunk_27.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos/chunk_27.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_23//scene_chunks/videos/chunk_27.0_4.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos/chunk_224.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos/chunk_224.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_24//scene_chunks/videos/chunk_224.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_144.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_144.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_91.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_91.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_222.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_222.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1280, 720])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_25//scene_chunks/videos/chunk_288.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_53.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_53.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_243.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_243.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_243.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_26//scene_chunks/videos/chunk_243.0_4.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_27//scene_chunks/videos/chunk_67.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_264.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_264.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_264.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_187.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_187.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_187.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_202.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_202.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_28//scene_chunks/videos/chunk_202.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_29//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_29//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_29//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_29//scene_chunks/labels\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/videos/chunk_188.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 720, 1280])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_3//scene_chunks/videos/chunk_188.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_4//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_4//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_4//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_4//scene_chunks/labels\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_5//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_5//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_5//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_5//scene_chunks/labels\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/videos/chunk_567.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_6//scene_chunks/videos/chunk_567.0_2.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_18.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_18.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_18.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_18.0_4.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_3.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_7//scene_chunks/videos/chunk_250.0_4.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos/chunk_161.0_1.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos/chunk_161.0_2.mp4\n",
      "samples dtype:torch.float32\n",
      "samples shape:torch.Size([90, 3, 1920, 1080])\n",
      "/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_8//scene_chunks/videos/chunk_161.0_3.mp4\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_9//scene_chunks/videos is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_9//scene_chunks/videos\n",
      "warning: the folder/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_9//scene_chunks/labels is not exist\n",
      "create folder /mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站//test_9//scene_chunks/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "/tmp/ipykernel_1788445/3037519057.py:5: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# root_path = '/mnt/ceph/develop/jiawei/lora_dataset/speech_data/B站/'\n",
    "# for root, dirs, files in os.walk(root_path):\n",
    "#     # 如果你只想获取下一层的子目录，可以在这里筛选\n",
    "#     if root == root_path:\n",
    "#         # root_dir 下的直接子目录就是 dirs 中的项\n",
    "#         for dir in dirs:\n",
    "#             split_video_with_chunk(root_path,dir)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
