{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ffa71c-27ab-4cc1-b075-134c365576b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 02:17:37,613 - modelscope - INFO - PyTorch version 2.0.1 Found.\n",
      "2023-09-11 02:17:37,615 - modelscope - INFO - TensorFlow version 2.10.0 Found.\n",
      "2023-09-11 02:17:37,615 - modelscope - INFO - Loading ast index from /home/dmeck/.cache/modelscope/ast_indexer\n",
      "2023-09-11 02:17:37,616 - modelscope - INFO - No valid ast index found from /home/dmeck/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2023-09-11 02:17:37,719 - modelscope - INFO - Loading done! Current index file version is 1.9.0, with md5 e74c3ad0d32f5b2c017304f8c2d8a5f8 and a total number of 921 components indexed\n",
      "2023-09-11 02:17:38,019 - modelscope - INFO - Model revision not specified, use the latest revision: v1.0.1\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 4.87k/4.87k [00:00<00:00, 490kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 126k/126k [00:00<00:00, 336kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1.11M/1.11M [00:00<00:00, 1.99MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 587k/587k [00:00<00:00, 862kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 65.2kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 597k/597k [00:00<00:00, 1.22MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 271k/271k [00:00<00:00, 1.65MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 170k/170k [00:00<00:00, 657kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 383k/383k [00:00<00:00, 992kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 6.07k/6.07k [00:00<00:00, 1.72MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1.52G/1.52G [02:32<00:00, 10.7MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10.3k/10.3k [00:00<00:00, 15.1MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 394/394 [00:00<00:00, 625kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:00<00:00, 68.7kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 107k/107k [00:00<00:00, 1.01MB/s]\n"
     ]
    }
   ],
   "source": [
    "#from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "#model_dir = snapshot_download('damo/multi-modal_clip-vit-large-patch14_336_zh',\n",
    "#                              cache_dir='/media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh',\n",
    "#                              revision='v1.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66de15ca-27b6-4fda-a331-598326888a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 22:48:17,583 - modelscope - INFO - PyTorch version 2.0.1 Found.\n",
      "2023-09-11 22:48:17,586 - modelscope - INFO - TensorFlow version 2.10.0 Found.\n",
      "2023-09-11 22:48:17,586 - modelscope - INFO - Loading ast index from /home/dmeck/.cache/modelscope/ast_indexer\n",
      "2023-09-11 22:48:17,661 - modelscope - INFO - Loading done! Current index file version is 1.9.0, with md5 e74c3ad0d32f5b2c017304f8c2d8a5f8 and a total number of 921 components indexed\n",
      "/media/gpt4-pdf-chatbot-langchain/pyenv-video/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-11 22:48:19.108475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 22:48:19.241121: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-11 22:48:19.672886: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /media/gpt4-pdf-chatbot-langchain/pyenv-video/lib/python3.9/site-packages/cv2/../../lib64::/opt/cuda/lib64:/opt/cuda/lib64\n",
      "2023-09-11 22:48:19.672958: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /media/gpt4-pdf-chatbot-langchain/pyenv-video/lib/python3.9/site-packages/cv2/../../lib64::/opt/cuda/lib64:/opt/cuda/lib64\n",
      "2023-09-11 22:48:19.672964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-09-11 22:48:20 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-09-11 22:48:20 | INFO | torch.distributed.nn.jit.instantiator | Created a temporary directory at /tmp/tmpfpfrpjft\n",
      "2023-09-11 22:48:20 | INFO | torch.distributed.nn.jit.instantiator | Writing /tmp/tmpfpfrpjft/_remote_module_non_scriptable.py\n",
      "2023-09-11 22:48:20,782 - modelscope - INFO - initiate model from /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh\n",
      "2023-09-11 22:48:20,783 - modelscope - INFO - initiate model from location /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh.\n",
      "2023-09-11 22:48:20,783 - modelscope - INFO - initialize model from /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh\n",
      "2023-09-11 22:48:20,784 - modelscope - INFO - Loading vision model config from /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh/vision_model_config.json\n",
      "2023-09-11 22:48:20,785 - modelscope - INFO - Loading text model config from /media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh/text_model_config.json\n",
      "2023-09-11 22:48:37,932 - modelscope - INFO - Use GPU 0 for finetuning & inference\n",
      "2023-09-11 22:48:37,934 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-09-11 22:48:37,934 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-09-11 22:48:37,934 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh'}. trying to build by task and model information.\n",
      "2023-09-11 22:48:37,935 - modelscope - WARNING - No preprocessor key ('clip-multi-modal-embedding', 'multi-modal-embedding') found in PREPROCESSOR_MAP, skip building preprocessor.\n"
     ]
    }
   ],
   "source": [
    "# require modelscope>=0.3.7，目前默认已经超过，您检查一下即可\n",
    "# 按照更新镜像的方法处理或者下面的方法\n",
    "# pip install --upgrade modelscope -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n",
    "# 需要单独安装decord，安装方法：pip install decord\n",
    "import torch\n",
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.preprocessors.image import load_image\n",
    "\n",
    "pipeline = pipeline(task=Tasks.multi_modal_embedding,\n",
    "    model='/media/checkpoint/multi-modal_clip-vit-large-patch14_336_zh/damo/multi-modal_clip-vit-large-patch14_336_zh', model_revision='v1.0.1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c37e9bd-e04a-4b04-a881-26f3e92e5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "'''\n",
    "获取输入文件夹内的所有jpg文件，并返回文件名全称列表\n",
    "'''\n",
    "def load_jpg(file_dir):\n",
    "    L = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == '.jpg':\n",
    "                filename=os.path.join(root, file)\n",
    "                L.append(filename)\n",
    "        return L \n",
    "        \n",
    "def batch(iterable, size):\n",
    "    # range对象的step是size\n",
    "    for i in range(0, len(iterable), size):\n",
    "        yield iterable[i:i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc53525d-0ee9-4c0c-ba0a-b37a9e9c2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取数据，成功1599\n"
     ]
    }
   ],
   "source": [
    "data_folder = '/media/checkpoint/speech_data/抖音作品/ieAeWyXU/keyframe-features'\n",
    "\n",
    "jpg_files=load_jpg(data_folder) \n",
    "print(\"获取数据，成功{}\".format(len(jpg_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981adb8e-7b19-49c9-95cd-816c05732bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1599/1599 [00:33<00:00, 47.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 768)\n",
      "(1599, 768)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm  # Import the tqdm function or class\n",
    "import numpy as np\n",
    " \n",
    "#对每一个文件进行操作\n",
    "batch_len=3\n",
    "batch_list = batch(jpg_files,batch_len)\n",
    "b_unit = tqdm(enumerate(jpg_files),total=len(jpg_files))\n",
    "\n",
    "img_embedding_list = []\n",
    "\n",
    "for batch_file in batch_list:  \n",
    "    table_data=[]\n",
    "    for filename in batch_file:\n",
    "        \n",
    "        input_img = load_image(filename)\n",
    "        # 支持一张图片(PIL.Image)或多张图片(List[PIL.Image])输入，输出归一化特征向量\n",
    "        # 2D Tensor, [图片数, 特征维度]\n",
    "        img_embedding = pipeline.forward({'img': input_img})['img_embedding']\n",
    "        #torch.Size([1, 768])形状转换 torch.Size([768])\n",
    "        tensor_squeezed = img_embedding.squeeze()\n",
    "        img_embedding_list.append(tensor_squeezed)\n",
    "    # 更新进度\n",
    "    b_unit.update(batch_len) \n",
    "\n",
    "\n",
    "# 循环结束后关闭进度条\n",
    "b_unit.close()\n",
    "\n",
    "save_name = os.path.join(data_folder, f\"features.pkl\")\n",
    "# Open the file in write mode\n",
    "with open(save_name, 'wb') as file:\n",
    "    \n",
    "    features = [tensor.cpu() for tensor in img_embedding_list]\n",
    "    features_np = [tensor.numpy() for tensor in features] \n",
    "    print(np.asarray(features_np).shape)\n",
    "    pickle.dump(features_np, file)\n",
    "    file.close\n",
    " \n",
    "\n",
    "\n",
    "# Read inference data from local\n",
    "with open(save_name, 'rb') as file:\n",
    "    features = pickle.load(file)\n",
    "\n",
    "\n",
    "features = np.asarray(features)\n",
    "\n",
    "print(features.shape)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3d049-3e3b-42b1-b8a1-eff74d5022da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
